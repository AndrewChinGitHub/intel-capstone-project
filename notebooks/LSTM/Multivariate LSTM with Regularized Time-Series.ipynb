{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # convert to datetime\n",
    "    df['MEASUREMENT_TIME'] = pd.to_datetime(df['MEASUREMENT_TIME'])\n",
    "    # calculate time difference\n",
    "    df['diff'] = df['MEASUREMENT_TIME'].diff().shift(-1)\n",
    "    # drop unncessary columns\n",
    "    df = df.drop(columns=['ID_INPUT', 'PRIVATE_DATA'])\n",
    "    # rename remaining columns\n",
    "    df.columns = ['time', 'window', 'diff']\n",
    "    # Select 3-week time-frame\n",
    "    df = df[(df['time'] > '2022-01-03') & (df['time'] <= '2022-01-24')]\n",
    "    # Remove windows appearing only once\n",
    "    df = df.groupby('window').filter(lambda x: len(x) > 1)\n",
    "    # Remove NaNs\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_timeseries(df, freq):\n",
    "    # regularized time-series index at specified frequency\n",
    "    out_time = pd.date_range(df['time'].values[0], df['time'].values[len(df)-1], freq=freq)\n",
    "    out_windows = []\n",
    "    \n",
    "    for i in range(len(out_time[:-1])):\n",
    "        # subquery dataframe to each time-step\n",
    "        df_small = df[(df['time'] >= out_time[i]) & (df['time'] <= out_time[i+1])]\n",
    "        \n",
    "        if len(df_small) == 0:\n",
    "            # NaN if no windows in time-step\n",
    "            out_windows.append(np.NaN)\n",
    "        elif len(df_small) == 1:\n",
    "            # append window if only one window in time-step\n",
    "            out_windows.append(df_small['window'].values[0])\n",
    "        else:     \n",
    "            # append window with most time spent if multiple windows during time-step\n",
    "            summed = df_small.groupby('window')['diff'].sum().reset_index().sort_values('diff', ascending=False)            \n",
    "            out_windows.append(summed['window'].values[0])\n",
    "    \n",
    "    # create new dataframe\n",
    "    out = pd.DataFrame(list(zip(out_time, out_windows)), columns =['time', 'window']).fillna(method=\"ffill\")\n",
    "    \n",
    "    return out.set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, n_unique):\n",
    "    \"\"\"one hot encode a sequence as 2-d array\"\"\"\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_unique)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return np.array(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(sequence, n_in, n_out):\n",
    "    \"\"\"transform encoded sequence to supervised learning problem\"\"\"\n",
    "    # create lag copies of the sequence\n",
    "    df = pd.DataFrame(sequence)\n",
    "    df = pd.concat([df.shift(n_in-i-1) for i in range(n_in)], axis=1)\n",
    "    # drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    # specify columns for input and output pairs\n",
    "    values = df.values\n",
    "    width = sequence.shape[1]\n",
    "    X = values.reshape(len(values), n_in, width)\n",
    "    y = values[:, 0:(n_out*width)].reshape(len(values), n_out, width)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"decode a one hot encoded string\"\"\"\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(pred, test, dim, enc):\n",
    "    \"\"\"decode all one-hot encoded strings and store as dataframe\"\"\"\n",
    "    preds = [] \n",
    "    for i in range(len(test[dim-1:])):\n",
    "        preds.append(one_hot_decode(pred[i])[0])\n",
    "        \n",
    "    return pd.DataFrame(enc.inverse_transform(preds), index=test.index[dim-1:], columns=['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_top_five():\n",
    "#     outputs = pd.read_csv('outputs/stats/all_model_outputs.csv')\n",
    "    \n",
    "# def make_plots():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_series_plot(df_train, df_valid, df_test, df_pred, freq):\n",
    "    \"\"\"Final App Launch Predictions / Total Time Spent\"\"\"\n",
    "    trace1 = go.Scatter(\n",
    "    x = df_train.index,\n",
    "    y = df_train.window,\n",
    "    mode = 'lines',\n",
    "    name = 'Training Data'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = df_valid.index,\n",
    "        y = df_valid.window,\n",
    "        mode = 'lines',\n",
    "        name = 'Validation Data'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x = df_test.index,\n",
    "        y = df_test.window,\n",
    "        mode = 'lines',\n",
    "        name = 'Testing Data'\n",
    "    )\n",
    "    trace4 = go.Scatter(\n",
    "        x = df_pred.index,\n",
    "        y = df_pred.window,\n",
    "        mode = 'lines',\n",
    "        name = 'Prediction'\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        title = \"App Launch Predictions at {} Intervals\".format(freq),\n",
    "        xaxis = {'title' : \"Time\"},\n",
    "        yaxis = {'title' : \"App Executable\"}\n",
    "    )\n",
    "    fig = go.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(df, user):\n",
    "    #number of models tested\n",
    "    count = 1\n",
    "    #regularization frequencies\n",
    "    freqs = [\"30s\", \"1min\", \"5min\"]\n",
    "    #parameters\n",
    "    look_backs = [3, 6, 10]\n",
    "    nodes = [8, 16, 32]\n",
    "    batch_sizes = [6, 12, 18]\n",
    "    #outputs dictionary\n",
    "    outputs = {'frequency': [], 'look_back':[], 'num_nodes':[], 'batch_size':[], \n",
    "               'accuracy':[], 'balanced_accuracy':[], 'weighted_f1_score':[], 'weighted_precision':[], 'weighted_recall':[]}\n",
    "    df = clean_data(df)\n",
    "    for freq in freqs:\n",
    "        #regularize time-series by frequency\n",
    "        df_regular = regularize_timeseries(df, freq)\n",
    "        #split regular data into train and test for later time-series evaluation\n",
    "        #60-20-20 split for train-validation-test\n",
    "        df_train_regular, df_test_regular = train_test_split(df_regular, test_size=0.4, shuffle=False)\n",
    "        df_valid_regular, df_test_regular = train_test_split(df_test_regular, test_size=0.5, shuffle=False)\n",
    "        #transform categorical labels to numerical\n",
    "        enc = LabelEncoder()\n",
    "        df_labeled = enc.fit_transform(df_regular)\n",
    "        encoded_length = len(enc.classes_)\n",
    "        #one-hot encode numerical labels\n",
    "        df_encoded = one_hot_encode(df_labeled, encoded_length)\n",
    "        #split one-hot encoded arrays into train and test\n",
    "        #60-20-20 split for train-validation-test\n",
    "        df_train, df_test = train_test_split(df_encoded, test_size=0.4, shuffle=False)\n",
    "        df_valid, df_test = train_test_split(df_test, test_size=0.5, shuffle=False)\n",
    "        for look_back in look_backs:\n",
    "            #specify different look_back values for sequence input and prediction length\n",
    "            #convert to supervised learning problem\n",
    "            X_train, y_train = to_supervised(df_train, look_back, look_back)\n",
    "            X_valid, y_valid = to_supervised(df_valid, look_back, look_back)\n",
    "            X_test, y_test = to_supervised(df_test, look_back, look_back)\n",
    "            #specify different number of nodes in LSTM layer\n",
    "            for node in nodes:\n",
    "                #specify different batch sizes for model fitting\n",
    "                for batch_size in batch_sizes:\n",
    "                    #instantiate Sequential model\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(node, input_shape=(look_back, encoded_length), return_sequences=True))\n",
    "                    model.add(TimeDistributed(Dense(encoded_length, activation='softmax')))\n",
    "                    model.compile(optimizer='adam', \n",
    "                                  loss='categorical_crossentropy', \n",
    "                                  metrics='categorical_accuracy')\n",
    "                    #optimal epochs selected based on epoch-loss curve\n",
    "                    history = model.fit(X_train, y_train, \n",
    "                              epochs=10, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(X_valid, y_valid),\n",
    "                              verbose=0, \n",
    "                              shuffle=False)\n",
    "                    #save model training logs in dataframe\n",
    "                    pd.DataFrame(history.history).to_csv(\"outputs/LSTM/stats/{}_model_{}_logs.csv\".format(user,count))\n",
    "                    #make predictions on test data\n",
    "                    y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "                    df_pred = decode_predictions(y_pred, df_test_regular, look_back, enc)\n",
    "                    fig = make_time_series_plot(df_train_regular, df_valid_regular, df_test_regular, df_pred, freq)\n",
    "                    fig.write_image(\"outputs/LSTM/plots/{}_model_{}_time_series_plot.png\".format(user,count))\n",
    "                    #save parameters and evaluate predictions\n",
    "                    outputs['frequency'].append(freq)\n",
    "                    outputs['look_back'].append(look_back)\n",
    "                    outputs['num_nodes'].append(node)\n",
    "                    outputs['batch_size'].append(batch_size)\n",
    "                    outputs['accuracy'].append(accuracy_score(df_pred, df_test_regular[look_back-1:]))\n",
    "                    outputs['balanced_accuracy'].append(balanced_accuracy_score(df_pred, df_test_regular[look_back-1:]))\n",
    "                    outputs['weighted_f1_score'].append(f1_score(df_pred, df_test_regular[look_back-1:], average='weighted'))\n",
    "                    outputs['weighted_precision'].append(precision_score(df_pred, df_test_regular[look_back-1:], average='weighted'))\n",
    "                    outputs['weighted_recall'].append(recall_score(df_pred, df_test_regular[look_back-1:], average='weighted'))\n",
    "                    print(count, end='\\r')\n",
    "                    count+=1\n",
    "    \n",
    "    pd.DataFrame(outputs).to_csv(\"outputs/LSTM/stats/{}_all_model_outputs.csv\".format(user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/user1_window_data.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_all_models(df1, \"user1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/user2_window_data.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_all_models(df2, \"user2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
