{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # convert to datetime\n",
    "    df['MEASUREMENT_TIME'] = pd.to_datetime(df['MEASUREMENT_TIME'])\n",
    "    # calculate time difference\n",
    "    df['diff'] = df['MEASUREMENT_TIME'].diff().shift(-1)\n",
    "    # drop unncessary columns\n",
    "    df = df.drop(columns=['ID_INPUT', 'PRIVATE_DATA'])\n",
    "    # rename remaining columns\n",
    "    df.columns = ['time', 'window', 'diff']\n",
    "    # Select 3-week time-frame\n",
    "    df = df[(df['time'] > '2022-01-03') & (df['time'] <= '2022-01-24')]\n",
    "    # Remove windows appearing only once\n",
    "    df = df.groupby('window').filter(lambda x: len(x) > 1)\n",
    "    # Remove NaNs\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_timeseries(df, freq):\n",
    "    # regularized time-series index at specified frequency\n",
    "    out_time = pd.date_range(df['time'].values[0], df['time'].values[len(df)-1], freq=freq)\n",
    "    out_windows = []\n",
    "    \n",
    "    for i in range(len(out_time[:-1])):\n",
    "        # subquery dataframe to each time-step\n",
    "        df_small = df[(df['time'] >= out_time[i]) & (df['time'] <= out_time[i+1])]\n",
    "        \n",
    "        if len(df_small) == 0:\n",
    "            # NaN if no windows in time-step\n",
    "            out_windows.append(np.NaN)\n",
    "        elif len(df_small) == 1:\n",
    "            # append window if only one window in time-step\n",
    "            out_windows.append(df_small['window'].values[0])\n",
    "        else:     \n",
    "            # append window with most time spent if multiple windows during time-step\n",
    "            summed = df_small.groupby('window')['diff'].sum().reset_index().sort_values('diff', ascending=False)            \n",
    "            out_windows.append(summed['window'].values[0])\n",
    "    \n",
    "    # create new dataframe\n",
    "    out = pd.DataFrame(list(zip(out_time, out_windows)), columns =['time', 'window']).fillna(method=\"ffill\")\n",
    "    \n",
    "    return out.set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, n_unique):\n",
    "    \"\"\"one hot encode a sequence as 2-d array\"\"\"\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_unique)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return np.array(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(sequence, n_in, n_out):\n",
    "    \"\"\"transform encoded sequence to supervised learning problem\"\"\"\n",
    "    # create lag copies of the sequence\n",
    "    df = pd.DataFrame(sequence)\n",
    "    df = pd.concat([df.shift(n_in-i-1) for i in range(n_in)], axis=1)\n",
    "    # drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    # specify columns for input and output pairs\n",
    "    values = df.values\n",
    "    width = sequence.shape[1]\n",
    "    X = values.reshape(len(values), n_in, width)\n",
    "    y = values[:, 0:(n_out*width)].reshape(len(values), n_out, width)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"decode a one hot encoded string\"\"\"\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(pred, test, dim, enc):\n",
    "    \"\"\"decode all one-hot encoded strings and store as dataframe\"\"\"\n",
    "    preds = [] \n",
    "    for i in range(len(test[n_in-1:])):\n",
    "        preds.append(one_hot_decode(pred[i])[0])\n",
    "        \n",
    "    return pd.DataFrame(enc.inverse_transform(preds), index=test.index[n_in-1:], columns=['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final App Launch Predictions / Total Time Spent\n",
    "def make_time_series_plot(df1, preds, freq):\n",
    "    trace1 = go.Scatter(\n",
    "    x = df1.index,\n",
    "    y = df1.window,\n",
    "    mode = 'lines',\n",
    "    name = 'Data'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = preds.index,\n",
    "        y = preds.window,\n",
    "        mode = 'lines',\n",
    "        name = 'Prediction'\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        title = \"App Launch Predictions at {} Frequency\".format(freq),\n",
    "        xaxis = {'title' : \"Time\"},\n",
    "        yaxis = {'title' : \"App Executable\"}\n",
    "    )\n",
    "    fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/arjun_window_data.csv')\n",
    "df1 = clean_data(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.window.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1_regular = regularize_timeseries(df1, \"1min\")\n",
    "df1_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1_regular.window.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "df1_labeled = enc.fit_transform(df1_regular)\n",
    "df1_encoded = one_hot_encode(df1_labeled, len(enc1.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User 1 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train, df1_test = train_test_split(df1_regular, test_size=0.4, shuffle=False)\n",
    "df1_valid, df1_test = train_test_split(df1_test, test_size=0.5, shuffle=False)\n",
    "df1_train_encoded, df1_test_encoded = train_test_split(df1_encoded, test_size=0.4, shuffle=False)\n",
    "df1_valid_encoded, df1_test_encoded = train_test_split(df1_test_encoded, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User 1 Encode for Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [\"1s\", \"30s\", \"1min\"]\n",
    "dims = [3,5,10]\n",
    "nodes = [8, 20, 100]\n",
    "batch_sizes = [6, 12, 18]\n",
    "encoded_length = len(enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1_train_X, df1_train_y = to_supervised(df1_train_encoded, dims[1], dims[1])\n",
    "df1_valid_X, df1_valid_y = to_supervised(df1_valid_encoded, dims[1], dims[1])\n",
    "df1_test_X, df1_test_y = to_supervised(df1_test_encoded, dims[1], dims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Fit + Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(nodes[1], input_shape=(dims[1], encoded_length), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(encoded_length, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy', 'poisson', 'kullback_leibler_divergence'])\n",
    "\n",
    "# train LSTM\n",
    "count = 0\n",
    "history = model.fit(df1_train_X, df1_train_y, \n",
    "                    epochs=2, \n",
    "                    batch_size=batch_sizes[1],\n",
    "                    validation_data=(df1_valid_X, df1_valid_y),\n",
    "                    verbose=1, \n",
    "                    shuffle=False, \n",
    "                    callbacks=[CSVLogger(\"outputs/model_{}.csv\".format(count))])\n",
    "\n",
    "# evaluate LSTM\n",
    "df1_pred_y = model.predict(df1_test_X, batch_size=batch_sizes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(df1_test_y, df1_pred_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pred = decode_predictions(df1_pred_y, df1_test, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_time_series_plot(df1_regular, df1_pred, \"1 Minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(df):\n",
    "    dfs = []\n",
    "    freqs = [\"1s\", \"30s\", \"1min\"]\n",
    "    dims = [3,5,10]\n",
    "    nodes = [8, 20, 100]\n",
    "    batch_sizes = [6, 12, 18]\n",
    "    \n",
    "    parameters = {'freq': [], 'dim':[], 'nodes':[], 'batch_size':[]}\n",
    "    outputs = {'pred_loss':[], 'pred_accuracy':[], 'pred_poisson':[], 'pred_kl_divergence':[]}\n",
    "    \n",
    "    df = clean_data(df)\n",
    "    \n",
    "    for freq in freqs:\n",
    "        df = regularize_timeseries(df, freq)\n",
    "        df.to_csv(\"data/regularized_{}.csv\".format(freq))\n",
    "        dfs.append(regularize_timeseries(df, freq))\n",
    "    \n",
    "    for df_i in dfs:\n",
    "        enc = LabelEncoder()\n",
    "        df_labeled = enc.fit_transform(df_i)\n",
    "        encoded_length = len(enc.classes_)\n",
    "        \n",
    "        df_encoded = one_hot_encode(df_labeled, encoded_length)\n",
    "        \n",
    "        df_train, df_test = train_test_split(df_encoded, test_size=0.4, shuffle=False)\n",
    "        df_valid, df_test = train_test_split(df_test, test_size=0.5, shuffle=False)\n",
    "        \n",
    "        for dim in dims:\n",
    "            X_train, y_train = to_supervised(df_train, dim, dim)\n",
    "            X_valid, y_valid = to_supervised(df_valid, dim, dim)\n",
    "            X_test, y_test = to_supervised(df_test, dim, dim)\n",
    "            \n",
    "            model = Sequential()\n",
    "            for node in nodes:\n",
    "                model.add(LSTM(node, input_shape=(dim, encoded_length), return_sequences=True))\n",
    "                model.add(TimeDistributed(Dense(encoded_length, activation='softmax')))\n",
    "\n",
    "                model.compile(optimizer='adam', \n",
    "                              loss='categorical_crossentropy', \n",
    "                              metrics=['categorical_accuracy', 'poisson', 'kullback_leibler_divergence'])\n",
    "\n",
    "                for batch_size in batch_sizes:\n",
    "                    history = model.fit(X_train, y_train, \n",
    "                              epochs=10, \n",
    "                              batch_size=batch_size, \n",
    "                              validation_data=(X_valid, y_valid),\n",
    "                              verbose=1, \n",
    "                              shuffle=False, \n",
    "                              callbacks=[CSVLogger(\"outputs/model_{}.csv\".format(count))])\n",
    "\n",
    "                    # evaluate LSTM\n",
    "                    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "                    output = model.evaluate(y_test, y_pred, verbose=1)\n",
    "                    \n",
    "                    outputs['dims'].append(dim)\n",
    "                    outputs['nodes'].append(node)\n",
    "                    outputs['batch_sizes'].append(batch_size)\n",
    "                    \n",
    "                    outputs['loss'].append(output[0])\n",
    "                    outputs['accuracy'].append(output[1])\n",
    "                    outputs['poisson'].append(output[2])\n",
    "                    outputs['kl_divergence'].append(output[3])\n",
    "    \n",
    "    pd.DataFrame(outputs)\n",
    "    \n",
    "    return pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_five():\n",
    "    parameters = pd.read_csv('outputs/')\n",
    "    models = pd.read_csv('data/')\n",
    "    outputs = pd.read_csv('outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/db0_to_39_window_data.csv')\n",
    "df2 = clean_data(df2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2_regular = regularize_timeseries(df2, \"1min\")\n",
    "df2_regular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
